---
layout: post
title: k近邻法
tags : [machine learning]
---

k近邻算法（knn，k nearest neighbor）是一个基本的分类算法。这个算法没有显示的机器学习过程。
knn算法事先要有已知分类的样本数据作为训练样本集，对于未知分类的样本数据，计算其与各个已知样本之间的距离，
取前k个距离最近的点，从这k个点中找到出现次数最多的分类作为新样本数据的分类。


knn法的计算过程
----
1. 给定训练数据集：$${(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)}$$，其中$$x_i$$为训练数据集和中节点$$i$$的
特征向量，$$y_i$$为对应节点的分类。
2. 根据距离的计算方式，对于给定一个新的节点$$x$$，从训练数据集中找到距离节点$$x$$最近的k个样本集和：$$N(k)$$。
3. 从上述的k个样本中找到拥有节点数最多的一个类别，即为测试样本的类别。

在knn的距离计算中方法中，一般会选取$$L_p$$距离计算方法。不同的距离计算方法，对于最后计算的k个最近节点有一定的影响。
k值的确定，对于模型的复杂度有影响。当k取较小的值时，新节点受近距离的节点影响较大，整个模型的复杂度较高，会
出现过拟合现象。如果k值较大会降低估计误差，但学习的近似误差会增大，使预测发生错误。k值增大使得模型变得简单。
应用中，k值一般取一个较小的数值，通常采用交叉验证方法选取最优k值。
knn的计算复杂度为O(nN+NlogN)。


knn算法的优化实现：kd树
----
knn的计算的最笨的办法是进行线性扫描，这在训练样本点数量很多的时候是根本不可行的。一个好的优化想法是对空间进行
划分和建立数据索引：kd树。

####kd树的构造####
给定k为空间数据集T，构造一个kd树

1. 构造根节点：构造一个包含数据集T的超矩形区域，对应于根节点
2. 选择$$x^1$$作为坐标轴，以T所有实例的$$x^1$$坐标的中位数为切分点，将根节点对应的超矩形区域切分为两部分，
切分通过通过切分点并与$$x^1$$坐标轴垂直的超平面实现。
3. 由根节点生成的深度为1的左右子节点：左子节点对应于坐标$$x^1$$小于根节点的子区域，右子节点对应于坐标$$x^1$$
大于切分点的子区域。
4. 对深度为j的节点，选择$$x^l$$为切分的坐标轴，$$l = j(mod k) + 1$$，以该节点的区域中所有实例的$$x^l$$的中
位数为切分点，将该节点对应的超矩形区域分为两个子区域，切分由通过切分点并与坐标轴$$x^l$$垂直的超平面实现。
5. 由该节点生成的深度为$$j+1$$的左右子节点，左子节点对应于坐标$$x^l$$小于根节点的子区域，右子节点对应于坐标$$x^1$$
大于切分点的子区域。
6. 落在切分超平面上的实例点保存在该节点
7. 知道两个子区域没有实例存在时停止。


####kd树的搜索####
利用kd树可以节省对大部分训练数据节点的搜索，减少计算量。
给定已经计算好的kd树，目标节点x，计算x的最近邻。

1. 在kd树中找到包含x的叶子节点：从根节点出发，递归向下访问kd树。若目标节点的当前维坐标切分点的坐标，则移动到
左子节点，否则移动到右子节点。直到子节点到达叶节点为止。
2. 以当前叶子节点作为当前最近点。
3. 递归的向上回退，对每个节点进行如下操作：
   * 如果该节点保存的实例地暖比当前最近点距离目标点更近，则以该实例点为当前最近点。
   * 当前最近点一定存在于该节点一个子节点对应的区域。检查该子节点的父节点的另一子节点对应的区域是否有更近的点
     。如果有，则移动到另一个子节点，递归的进行最近邻搜索。如果没有，则接着向上回退
4. 当回退到根节点时，搜索结束。最后的“当前最近点”即为x的最近邻点。


knn的应用
----
knn主要应用于：SIFT算法，SIFT是一种电脑视觉的算法用来侦测与描述影像中的局部性特征，其应用范围
包含物体辨识、机器人地图感知与导航、影像缝合、3D模型建立、手势辨识、影像追踪和动作比对。


knn的开源算法包
----
1. python的科学计算库[scipy](http://wiki.scipy.org/Cookbook/KDTree)中包含了对于kd树的构造和搜索
2. [libkdtree++](http://libkdtree.alioth.debian.org/)是一个c++版的kd树实现语言包